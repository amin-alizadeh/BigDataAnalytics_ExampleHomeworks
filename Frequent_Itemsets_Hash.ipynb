{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readdata(k, fname=\"data.txt\", report=False, min_length=1):\n",
    "    C_k = []\n",
    "    b = 0\n",
    "    \n",
    "    with open(fname, \"rt\", encoding='latin1') as f:\n",
    "        for line in f:\n",
    "            # report progress\n",
    "            # print every 1000th element to reduce clutter\n",
    "            if report:\n",
    "                if b % 1000 == 0:  \n",
    "                    print('processing bin ', b)\n",
    "                b += 1\n",
    "            line = line.replace('\\n', '')  # remove newline symbol\n",
    "            C_k = list(filter(lambda x: len(x) > min_length, line.split(' ')))\n",
    "            #to save time for sentences shorter than k words\n",
    "            if len(C_k) >= k:\n",
    "                for itemset in itertools.combinations(C_k, k):\n",
    "                    #to eliminate duplicate words which would result in smaller tuples\n",
    "                    _set = frozenset(itemset)\n",
    "                    if (len(_set)) == k:\n",
    "                        yield _set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_itemsets(k, N, hash_size, fname, previous_hash_set=None, previous_hash_size=None, return_item_set=False, min_length=1):\n",
    "    if previous_hash_size is None:\n",
    "        previous_hash_size = hash_size\n",
    "        \n",
    "    C = {}\n",
    "    L = {}\n",
    "    H = np.zeros((hash_size,), dtype=np.int)\n",
    "\n",
    "    for key in readdata(k=k, fname=fname, report=False, min_length=min_length):\n",
    "        #print(\"\\nKey\", key)\n",
    "        frequent_items_count = 0\n",
    "        if k > 1:\n",
    "            for itemset in itertools.combinations(key, k-1):\n",
    "                _set = frozenset(list(itemset))\n",
    "                #print(_set, itemset)\n",
    "                if (hash(_set) % previous_hash_size) in previous_hash_set:\n",
    "                    #print(\"Occurence,\", _key)\n",
    "                    frequent_items_count += 1\n",
    "        if frequent_items_count == k or k == 1:\n",
    "            if return_item_set:\n",
    "                if key not in C:\n",
    "                    C[key] = 1\n",
    "                else:\n",
    "                    C[key] += 1\n",
    "            else:\n",
    "                hash_cell = hash(key) % hash_size\n",
    "                H[hash_cell] += 1\n",
    "    #filtering\n",
    "    if return_item_set:\n",
    "        for key, count in C.items():\n",
    "            if count >= N:\n",
    "                L[key] = count\n",
    "        del C\n",
    "        return L\n",
    "    else:\n",
    "        H_good = set(np.where(H >= N)[0])\n",
    "        del H\n",
    "        return H_good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most frequent 3-itemsets for *NSF_abstract_sentences*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166 items with more than 3000 occurrences\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "N = 3000  # frequency threshold\n",
    "min_length = 1 #to skip words whose length is 1 or less\n",
    "fname = 'NSF_abstract_sentences.txt'\n",
    "TopN = 10\n",
    "hash_size = 100000\n",
    "k=1\n",
    "H = get_itemsets(k=k, N=N, hash_size=hash_size, fname=fname, previous_hash_set=None, previous_hash_size=None, return_item_set=False, min_length=min_length)\n",
    "print(\"{} items with more than {} occurrences\".format(len(H), N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9169 items with more than 3000 occurrences\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hash_size2 = 1000000\n",
    "k=2\n",
    "H2 = get_itemsets(k=k, N=N, hash_size=hash_size2, fname=fname, previous_hash_set=H, previous_hash_size=hash_size, return_item_set=False, min_length=min_length)\n",
    "print(\"{} items with more than {} occurrences\".format(len(H2), N))\n",
    "del H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30614 items with more than 3000 occurrences\n",
      "Wall time: 4h 11min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hash_size3 = 1000000\n",
    "k=3\n",
    "C = get_itemsets(k=k, N=N, hash_size=hash_size3, fname=fname, previous_hash_set=H2, previous_hash_size=hash_size2, return_item_set=True, min_length=min_length)\n",
    "print(\"{} items with more than {} occurrences\".format(len(C), N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 most frequent itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5705231 occurences for: of, and, the\n",
      "3318151 occurences for: the, of, to\n",
      "3035874 occurences for: in, the, of\n",
      "2658111 occurences for: to, the, and\n",
      "2327955 occurences for: in, the, and\n",
      "1947722 occurences for: to, of, and\n",
      "1787502 occurences for: in, of, and\n",
      "1407020 occurences for: in, the, to\n",
      "1321118 occurences for: of, for, the\n",
      "1320281 occurences for: will, of, the\n"
     ]
    }
   ],
   "source": [
    "#TopN items\n",
    "most_frequent_TopN = sorted(C.items(), key=lambda k: -k[1])[:TopN]\n",
    "for k, v in most_frequent_TopN:\n",
    "    print(\"{} occurences for: {}\".format(v, ', '.join(list(k))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Most frequent 3-itemsets for *NSF_abstract_sentences_nostopwords*\n",
    "## 1-itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1214 items with more than 2000 occurrences\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "N = 2000  # frequency threshold\n",
    "fname = 'NSF_abstract_sentences_nostopwords.txt'\n",
    "hash_size = 100000\n",
    "k=1\n",
    "H = get_itemsets(k=k, N=N, hash_size=hash_size, fname=fname, previous_hash_set=None, previous_hash_size=None, return_item_set=False)\n",
    "print(\"{} items with more than {} occurrences\".format(len(H), N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284 items with more than 2000 occurrences\n",
      "Wall time: 4min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hash_size2 = 1000000\n",
    "k=2\n",
    "H2 = get_itemsets(k=k, N=N, hash_size=hash_size2, fname=fname, previous_hash_set=H, previous_hash_size=hash_size, return_item_set=False)\n",
    "print(\"{} items with more than {} occurrences\".format(len(H2), N))\n",
    "del H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 items with more than 2000 occurrences\n",
      "Wall time: 26min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hash_size3 = 1000000\n",
    "k=3\n",
    "C = get_itemsets(k=k, N=N, hash_size=hash_size3, fname=fname, previous_hash_set=H2, previous_hash_size=hash_size2, return_item_set=True)\n",
    "print(\"{} items with more than {} occurrences\".format(len(C), N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 most frequent itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4410 occurences for: school, students, high\n",
      "4354 occurences for: science, students, mathematics\n",
      "4096 occurences for: science, students, engineering\n",
      "3425 occurences for: science, computer, engineering\n",
      "3370 occurences for: science, education, mathematics\n",
      "3238 occurences for: science, students, program\n",
      "3226 occurences for: science, teachers, mathematics\n",
      "3113 occurences for: division, chemistry, program\n",
      "3004 occurences for: differential, partial, equations\n",
      "2993 occurences for: science, school, high\n"
     ]
    }
   ],
   "source": [
    "''#TopN items\n",
    "most_frequent_TopN = sorted(C.items(), key=lambda k: -k[1])[:TopN]\n",
    "for k, v in most_frequent_TopN:\n",
    "    print(\"{} occurences for: {}\".format(v, ', '.join(list(k))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "## Including Stop Words vs. Excluding Stop Words\n",
    "The top 10 most frequent 3-itemset of the text file which includes stop words consists of only stop words suchs as _and, the, of, in, will, in_. Whereas the list of the top 10 most frequent 3-itemset of the file without the stop words consists of other common words that are expected to be used in scientific publications in English language.\n",
    "Also by comparing the number of occurrences, it is obvious that the stop words are very common. For instance the most common itemset in the complete file (of, and, the) occurs 5705231 times. On the other hand the most frequent itemset _{school, students, high}_ of the non-stop word file only occurs 4410 times.\n",
    "The results are:\n",
    "- **With stop words**:\n",
    " - 5705231 occurences for: of, and, the\n",
    " - 3318151 occurences for: the, of, to\n",
    " - 3035874 occurences for: in, the, of\n",
    " - 2658111 occurences for: to, the, and\n",
    " - 2327955 occurences for: in, the, and\n",
    " - 1947722 occurences for: to, of, and\n",
    " - 1787502 occurences for: in, of, and\n",
    " - 1407020 occurences for: in, the, to\n",
    " - 1321118 occurences for: of, for, the\n",
    " - 1320281 occurences for: will, of, the\n",
    "- **Without stop words**:\n",
    " - 4410 occurences for: school, students, high\n",
    " - 4354 occurences for: science, students, mathematics\n",
    " - 4096 occurences for: science, students, engineering\n",
    " - 3425 occurences for: science, computer, engineering\n",
    " - 3370 occurences for: science, education, mathematics\n",
    " - 3238 occurences for: science, students, program\n",
    " - 3226 occurences for: science, teachers, mathematics\n",
    " - 3113 occurences for: division, chemistry, program\n",
    " - 3004 occurences for: differential, partial, equations\n",
    " - 2993 occurences for: science, school, high\n",
    "\n",
    "## Effect of considering *min_length* of a word\n",
    "In the first attempt I included all 1 letter words that resulted in detecting a high frequency of vriables such as _x, y, n_, etc. In this report I excluded 1 letter words by applying a filter. \n",
    "The results of setting *min_length=0*:\n",
    "- **With stop words**:\n",
    " - 9838559 occurences for: y, d, z\n",
    " - 8134492 occurences for: y, d, c\n",
    " - 7543647 occurences for: d, c, z\n",
    " - 7102632 occurences for: y, d, x\n",
    " - 6629631 occurences for: d, x, z\n",
    " - 5705231 occurences for: and, of, the\n",
    " - 5545656 occurences for: d, c, x\n",
    " - 3710611 occurences for: y, d, o\n",
    " - 3588119 occurences for: y, n, d\n",
    " - 3355157 occurences for: n, z, d\n",
    "- **Without stop words**:\n",
    " - 766546 occurences for: n, c, x\n",
    " - 385234 occurences for: k, c, x\n",
    " - 324834 occurences for: c, x, g\n",
    " - 301788 occurences for: c, x, p\n",
    " - 259471 occurences for: c, h, x\n",
    " - 194641 occurences for: x, c, j\n",
    " - 182833 occurences for: k, x, p\n",
    " - 172291 occurences for: c, f, x\n",
    " - 167445 occurences for: n, c, g\n",
    " - 155797 occurences for: c, x, l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
